````markdown
# Company AI Chatbot (Gradio + RAG)

A conversational chatbot UI built with Gradio that lets users interact with your companyâ€™s AI knowledge base, powered by Retrieval-Augmented Generation (RAG) using Google Gemini embeddings and ChromaDB.

---

## ğŸš€ Features

- **Retrieval-Augmented Generation**  
  â€¢ Indexes your company policy/docs as chunks in a vector store  
  â€¢ On-demand retrieval of relevant context for each user query  
  â€¢ Goldenâ€‘tone â€œGâ€‘Botâ€ persona for friendly, professional responses  

- **Gradio Chat Interface**  
  â€¢ Simple, webâ€‘based chat UI  
  â€¢ â€œShareâ€ mode for public demos  
  â€¢ Customizable theme and layout  

- **Modular & Extensible**  
  â€¢ Pluggable document parsers, embedding services, and vector stores  
  â€¢ Add new document types (PDF, Word) or switch embedding providers  
  â€¢ Easily adjust chunk size, overlap, and number of retrieved results  

---

## ğŸ“¦ Prerequisites

- Python 3.10+  
- Google Gemini API Key (for embeddings & generation)  
- `chromadb` (for vector database)  
- A folder of Markdown (or other) policy/docs under `./source_data`

---

## ğŸ”§ Installation

1. **Clone the repo**  
   ```bash
   git clone https://github.com/your-org/company-ai-chatbot.git
   cd company-ai-chatbot
````

2. **Create & activate a virtual environment**

   ```bash
   python3 -m venv .venv
   source .venv/bin/activate   # macOS/Linux
   .venv\Scripts\activate      # Windows
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Set your environment variables**
   Create a `.env` file in project root:

   ```env
   GEMINI_API_KEY=your_google_gemini_api_key_here
   ```

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ .env
â”œâ”€â”€ main.py                   # Entry point: sets up Gradio and bot initialization
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ai_assistance.py      # CompanyAssistanceBot: RAG orchestration
â”‚   â”œâ”€â”€ document_parsing.py   # DocumentProcessor: load & chunk markdown files
â”‚   â”œâ”€â”€ data_embedding.py     # EmbeddingService: Google Gemini embedding calls
â”‚   â”œâ”€â”€ data_vectorizing.py   # VectorStore: chromadb storage & search
â”‚   â””â”€â”€ utils/                # (Optional) helper modules
â”‚       â””â”€â”€ ...
â”œâ”€â”€ source_data/              # Your company policy/docs (Markdown files)
â”‚   â””â”€â”€ *.md
â””â”€â”€ README.md
```

---

## âš™ï¸ Configuration

| Setting         | Location               | Description                                                |
| --------------- | ---------------------- | ---------------------------------------------------------- |
| `chunk_size`    | `CompanyAssistanceBot` | Max words per chunk (default: 500)                         |
| `chunk_overlap` | `DocumentProcessor`    | Overlap between chunks (default: 50)                       |
| `top_k`         | `CompanyAssistanceBot` | Number of top documents to retrieve per query (default: 3) |
| `theme`         | `gr.ChatInterface`     | Gradio theme (e.g., `ocean`, `default`)                    |
| `share`         | `.launch(share=True)`  | Whether to enable Gradioâ€™s public share link               |

---

## ğŸš€ Usage

1. **Prepare your documents**
   Place all your Markdown policy/docs in `./source_data/`.
   Example:

   ```
   source_data/
   â”œâ”€â”€ Employee_Handbook.md
   â”œâ”€â”€ Code_of_Conduct.md
   â””â”€â”€ IT_Policies.md
   ```

2. **Index documents**
   On first run, the bot will automatically chunk, embed, and store vectors in `./chroma_db/`.

   ```bash
   python main.py
   ```

   You should see console logs:

   ```
   Initializing FAQ Bot...
   Loading documents...
   Processing documents into chunks...
   Generating embeddings...
   Storing in vector database...
   Indexed 150 chunks from 5 documents
   ```

3. **Chat with Gâ€‘Bot!**
   After successful indexing, Gradio will launch a local web UI.

   * **Local:** `http://127.0.0.1:7860/`
   * **Share Link:** As printed in the console

4. **Ask questions**
   ğŸ’¬ â€œWhatâ€™s our PTO policy?â€
   ğŸ’¬ â€œHow do I reset my corporate VPN password?â€

---

## ğŸ“ˆ How it Works

1. **Document Parsing**

   * `DocumentProcessor` loads each `.md` file and splits text into overlapping chunks.
2. **Embedding Generation**

   * `EmbeddingService` calls Google Gemini to convert text chunks (and user queries) into highâ€‘dimensional vectors.
3. **Vector Store**

   * `VectorStore` (ChromaDB) indexes embeddings & metadata for fast similarity search.
4. **RAG Workflow**

   * On each user query:

     1. Embed the question
     2. Retrieve topâ€‘k relevant chunks
     3. Feed context + question into the Gemini generative model
     4. Return a coherent, policyâ€‘aware answer

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/awesome`)
3. Commit your changes (`git commit -m "Add awesome feature"`)
4. Push (`git push origin feature/awesome`)
5. Open a Pull Request

Please follow the existing code style and write tests for new functionality.

---

*Happy chatting with your Company AI! ğŸ‰*

```
```
